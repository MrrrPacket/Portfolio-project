{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106a61ed",
   "metadata": {},
   "source": [
    "# Задача:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d42921",
   "metadata": {},
   "source": [
    "Исследование и анализ популярных тематик в корпусе текстов-комментариев из видео на тему финтеха."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03337ba6",
   "metadata": {},
   "source": [
    "# Гипотеза:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5e06b6",
   "metadata": {},
   "source": [
    "Существует определенная группа высокочастотных токенов и тем, которые регулярно встречаются в текстах, связанных с видео, и их идентификация может помочь в определении популярных тематик в данном корпусе.\n",
    "\n",
    "- Н0: подтверждена\n",
    "- Н1: опровержена"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42557915",
   "metadata": {},
   "source": [
    "# Обоснование метода:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246e223",
   "metadata": {},
   "source": [
    "Для анализа корпуса текстов будет использоваться метод pymystem3 и Scikit-learn. \n",
    "При помощи первого будет произведена предобработка текста, токенизация, подсчет частоты встречаемости токенов.\n",
    "При помощи второго будет произведена векторизация."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b483838e",
   "metadata": {},
   "source": [
    "# Выгрузка не менее сотни (если не обосновано иное) недублирующихся (по их id или содержанию) текстов на каждого участника команды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ad2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Импорт необходимых библиотек\n",
    "import googleapiclient.discovery as api, pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1ddd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Конструкция обращения к АПИ ютуба\n",
    "API_KEY = \"AIzaSyASvmJMngF2Q4jLrJm_BD4MLkt8U4sym2U\"\n",
    "api_service_name = \"youtube\"\n",
    "api_version = \"v3\"\n",
    "\n",
    "youtube = api.build(api_service_name,\n",
    "    api_version,\n",
    "    developerKey= API_KEY)\n",
    "# Прописываем переменные, которые будут исопльзоваться\n",
    "maxResults = 100\n",
    "order = 'relevance'\n",
    "part = 'id, replies, snippet'\n",
    "textFormat = 'plainText'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc39748e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Создаём переменную с Id имеющихся видео (все видео были вручную отфильтрованы на предмет релевантности исследованию)\n",
    "videossId = pandas.read_excel('Все_видео.xlsx', usecols='B')\n",
    "videossId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabef5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Пробный запрос с одним из видео.\n",
    "request = youtube.commentThreads().list(\n",
    "    part=part,\n",
    "    maxResults=maxResults,\n",
    "    videoId=\"wmbLep9dW8U\"\n",
    ")\n",
    "response = request.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598d8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Пропустим через цикл. Для начала формируем список видео, с которыми у нас есть проблемы. Основные процессы визуализированы\n",
    "problem_videoId = []\n",
    "# Комментарии выводим в датафреме\n",
    "comments = pandas.DataFrame()\n",
    "# Создаём условие, при котором видео с ошибками будут помещаться в список problem_videoId при помощи метода append, а \"нормальные\" формировать содержимое comments\n",
    "for videoId in videossId['id']:\n",
    "    try:\n",
    "        request = youtube.commentThreads().list(\n",
    "            part=part,\n",
    "            videoId=videoId,\n",
    "            maxResults=maxResults\n",
    "        )\n",
    "        response = request.execute()\n",
    "        comments_additional = pandas.json_normalize(response['items'])\n",
    "        comments = pandas.concat([comments, comments_additional])\n",
    "        print(f\"Видео №{videossId[videossId['id'] == videoId].index[0]} из {len(videossId) - 1}\")\n",
    "        i = 1\n",
    "        while 'nextPageToken' in response.keys():\n",
    "            request = youtube.commentThreads().list(\n",
    "                part=part,\n",
    "                videoId=videoId,\n",
    "                maxResults=maxResults,\n",
    "                pageToken=response['nextPageToken']\n",
    "            )\n",
    "            response = request.execute()\n",
    "            comments_additional = pandas.json_normalize(response['items'])\n",
    "            comments = pandas.concat([comments, comments_additional])\n",
    "            print(f'Итерация №{i}')\n",
    "            i += 1\n",
    "\n",
    "    except:\n",
    "        print(f\"Видео №{videossId[videossId['id'] == videoId].index[0]} -- проблема\")\n",
    "        problem_videoId.append(videoId)\n",
    "\n",
    "comments = comments.drop_duplicates('id')\n",
    "comments = comments.drop(['snippet.topLevelComment.kind',\n",
    "                          'snippet.topLevelComment.etag',\n",
    "                          'snippet.topLevelComment.id',\n",
    "                          'snippet.topLevelComment.snippet.videoId'], axis=1)\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1af4d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Обновляем индекс\n",
    "comments.index = range(1, len(comments) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098e43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Сбрасываем дубликаты\n",
    "comments.drop_duplicates('id')['snippet.totalReplyCount'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec3d276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Посмотрим только на те комменатрии, на которые отвечали\n",
    "comments[comments['snippet.totalReplyCount'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50249aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Запишем номера отвеченных комментариев\n",
    "comments_repliesIndex = comments[comments['snippet.totalReplyCount'] > 0].index\n",
    "comments_repliesIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f79193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Создадим новый столбец, в который будет записываться недостача\n",
    "comments.loc[comments_repliesIndex[0], 'Недостача_ответов'] = comments['snippet.totalReplyCount'][comments_repliesIndex[0]]\\\n",
    "- len(pandas.json_normalize(comments['replies.comments'][comments_repliesIndex[0]]))\n",
    "comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138db4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Записываем раннее выгруженный комменатрий в таблицу с ответами\n",
    "replies = pandas.json_normalize(comments['replies.comments'][comments_repliesIndex[0]])\n",
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f23cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Тепеь пройдёмся по всем родительским комментариям, у которых есть ответы и распарсим эти ответы, сохраняя их каждый раз в датафреймы с последующим объединением через конкат\n",
    "comments.index = range(1, len(comments) + 1)\n",
    "\n",
    "comments_repliesIndex = comments[comments['snippet.totalReplyCount'] > 0].index\n",
    "\n",
    "replies = pandas.DataFrame()\n",
    "j = 1\n",
    "\n",
    "for i in comments_repliesIndex:\n",
    "    comments.loc[i, 'Недостача_ответов'] = comments['snippet.totalReplyCount'][i]\\\n",
    "    - len(pandas.json_normalize(comments['replies.comments'][i]))\n",
    "    \n",
    "    replies_additional = pandas.json_normalize(comments['replies.comments'][i])\n",
    "    replies = pandas.concat([replies, replies_additional])\n",
    "    \n",
    "    print(f\"Итерация {j} из {len(comments_repliesIndex)}\")\n",
    "    j += 1\n",
    "    \n",
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ae3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#13. Проверим, есть ли совпадающие столбцы в таблицах родительских комментариев и ответов\n",
    "mutualColumns = []\n",
    "for column in comments.columns:\n",
    "    if column in replies.columns:\n",
    "        mutualColumns.append(column)\n",
    "mutualColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Избавляемся от префиксов\n",
    "comments_replies = comments.copy()\n",
    "\n",
    "comments_replies_new_columns = []\n",
    "for column in comments_replies.columns:\n",
    "    if 'snippet.topLevelComment.' in column:\n",
    "        column = column.replace('snippet.topLevelComment.', '')\n",
    "    comments_replies_new_columns.append(column)\n",
    "comments_replies_new_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacd211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Присваиваем новые названия столбцам таблицы:\n",
    "comments_replies.columns = comments_replies_new_columns\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3abb30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16. Перепроверяем на наличие совпадений\n",
    "mutualColumns = []\n",
    "for column in comments_replies.columns:\n",
    "    if column in replies.columns:\n",
    "        mutualColumns.append(column)\n",
    "mutualColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd31efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#17. Процесс объединения\n",
    "replies.loc[:, 'snippet.totalReplyCount'] = 0\n",
    "replies.loc[:, 'Недостача_ответов'] = 0\n",
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2e85e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18. Переперепроверяем на наличие совпадений\n",
    "mutualColumns = []\n",
    "for column in comments_replies.columns:\n",
    "    if column in replies.columns:\n",
    "        mutualColumns.append(column)\n",
    "mutualColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d3eedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19. Оставляем только совпадающие столбцы таблицы с род. комменатриями\n",
    "comments_replies = comments_replies[mutualColumns]\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20. То же самое только с комментариями-ответами\n",
    "replies = replies[mutualColumns]\n",
    "replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baab795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21. Объединим обе таблицы с конкатом\n",
    "comments_replies = pandas.concat([comments_replies, replies])\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c470810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 22. Сколько комментариев ютуб не выдал?\n",
    "comments_replies['Недостача_ответов'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c36583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 23. выписываем нужные переменные\n",
    "part = 'id, snippet'\n",
    "\n",
    "id = ''\n",
    "parentId = ''\n",
    "\n",
    "maxResults = 100\n",
    "\n",
    "order = 'relevance'\n",
    "\n",
    "pageToken = ''\n",
    "\n",
    "textFormat = 'plainText'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58cb329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24. Пришла пора выгрузить недосдачу по комментариям\n",
    "\n",
    "print(f\"Сколько комментариев-ответов YouTube не додал? {comments_replies['Недостача_ответов'].sum()}\")\n",
    "\n",
    "print(f\"Сколько всего ожидается ответов на интересующие родительские (topLevel) комментарии? \\\n",
    "{comments_replies['snippet.totalReplyCount'][comments_replies['Недостача_ответов'] > 0].sum()}\")\n",
    "\n",
    "problem_commentId = []\n",
    "replies = pandas.DataFrame()\n",
    "j = 1\n",
    "# Создаём цикл\n",
    "for commentId in comments_replies['id'][comments_replies['Недостача_ответов'] > 0]:\n",
    "    \n",
    "    try:\n",
    "        request = youtube.comments().list(\n",
    "            part=part,\n",
    "            parentId=commentId,\n",
    "            maxResults=maxResults\n",
    "        )\n",
    "        response = request.execute()\n",
    "        \n",
    "        # Процесс записи содержимого в таблицу\n",
    "        replies_additional = pandas.json_normalize(response['items'])\n",
    "        replies = pandas.concat([replies, replies_additional])\n",
    "        \n",
    "        # Визуализируем процесс\n",
    "        print(f\"Комментарий №{j} из {len(comments_replies['id'][comments_replies['Недостача_ответов'] > 0]) }\")\n",
    "        j += 1\n",
    "        \n",
    "        # Создаём цикл для прохода по всем следующим страницам выдачи с процедурой записи в таблицу\n",
    "        i = 1\n",
    "        while 'nextPageToken' in response.keys():\n",
    "            request = youtube.comments().list(\n",
    "                part=part,\n",
    "                parentId=commentId,\n",
    "                maxResults=maxResults,\n",
    "                pageToken=response['nextPageToken']\n",
    "            )\n",
    "            response = request.execute()\n",
    "        \n",
    "            # Процесс записи содержимого в таблицу\n",
    "            replies_additional = pandas.json_normalize(response['items'])\n",
    "            replies = pandas.concat([replies, replies_additional])\n",
    "            \n",
    "            print(f\"Итерация №{i}\")\n",
    "            i += 1\n",
    "            \n",
    "    except:\n",
    "        print(f\"Видео №{comments_replies[comments_replies['id'] == commentId].index[0]} -- проблема\")\n",
    "        \n",
    "        problem_commentId.append(commentId)\n",
    "        \n",
    "print(f\"Ответов {len(replies)}, а проблемных родительских (topLevel) комментариев {len(problem_commentId)}\")\n",
    "\n",
    "# Добавляем столбцы 'snippet.totalReplyCount' и 'Недостача_ответов' в таблицу с комментариями-ответами\n",
    "replies.loc[:, 'snippet.totalReplyCount'] = 0\n",
    "replies.loc[:, 'Недостача_ответов'] = 0\n",
    "\n",
    "# Удаляем столбец snippet.parentId, т.к. есть \"id\"\n",
    "replies = replies.drop('snippet.parentId', axis=1)\n",
    "\n",
    "# Объединяем обе таблицы\n",
    "comments_replies = pandas.concat([comments_replies, replies])\n",
    "\n",
    "comments_replies = comments_replies.drop_duplicates('id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4564500",
   "metadata": {},
   "source": [
    "# Предобработка текстов: удаление «мусорных» (в контексте решаемой задачи) символов, лемматизация слов (приведение их к начальной лексической форме), удаление стоп-слов (высокочастотных, но НЕ несущих значимый смысл в контексте решаемой задачи), векторизация документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562960ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25. Импорт библиотек\n",
    "import pymystem3, stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68fdda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Выведение базы данных\n",
    "comments_replies.index = range(1,len(comments_replies)+1)\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0e3c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 27. Выбираем столбец для предобработки\n",
    "data = pandas.DataFrame()\n",
    "data.loc[:,'Текст_на_предобработку'] = comments_replies['snippet.textOriginal']\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c13430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Удаляем пропуски в ячейках -- их не оказалось, \n",
    "# но в случае если бы мы работали с тегами, такое может быть\n",
    "data = data[data['Текст_на_предобработку'].notna()]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73db57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28.1. Убираем \"мусорные\" символы\n",
    "data['Текст_на_предобработку'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9eac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28.2. Очищаем объект \n",
    "cleaned_text = ''\n",
    "for simbol in data['Текст_на_предобработку'][2]:\n",
    "    if (simbol.isalnum()) | (simbol == ' '):\n",
    "        cleaned_text = cleaned_text + simbol\n",
    "    else:\n",
    "        cleaned_text = cleaned_text + ' '\n",
    "cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f938705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28.3. Создаём функцию для очистики комментариев от мусора\n",
    "def simbols_cleaner(text):\n",
    "    cleaned_text = ''\n",
    "    for simbol in text:\n",
    "        if (simbol.isalnum()) | (simbol == ' '):\n",
    "            cleaned_text = cleaned_text + simbol\n",
    "        else:\n",
    "            cleaned_text = cleaned_text + ' '\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ff7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28.4. Применим функцию ко всем ячейкам датафрейма с комментариями\n",
    "data['Текст_на_предобработку'].apply(simbols_cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb5610d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28.5. Записываем результат\n",
    "data['Предобработанный_текст'] = data['Текст_на_предобработку'].apply(simbols_cleaner)\n",
    "data['Предобработанный_текст']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd984b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 29. Посмотрим на датафрейм\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d6a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30. Обращаемся к пакету pymystem3\n",
    "mstem = pymystem3.Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30.1. Применяем лемматизацию ко всем ячейкам нужного столбца\n",
    "data['Предобработанный_текст'] = data['Предобработанный_текст'].apply(mstem.lemmatize)\n",
    "data['Предобработанный_текст']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f2311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31. Обращаемся к нужному методу для удаления стоп-слов\n",
    "display(stop_words.get_stop_words('russian'),\n",
    "        stop_words.get_stop_words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6a21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31.1. Создаём общий список стоп-слов и по желанию добавляем свои слова или удаляем ненужные при помощи remove\n",
    "stopwords_list = stop_words.get_stop_words('russian')\n",
    "stopwords_list.extend(stop_words.get_stop_words('english'))\n",
    "stopwords_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478b22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31.2. Оформляем функцию для работы со всеми лемметизированными комментариями\n",
    "def words_cleaner(text):\n",
    "    text_cleaned = ''\n",
    "    for word in text:\n",
    "        if word not in stopwords_list:\n",
    "            text_cleaned += word\n",
    "            \n",
    "    while '  ' in text_cleaned:\n",
    "        text_cleaned = text_cleaned.replace('  ', ' ')\n",
    "        \n",
    "    text_cleaned = text_cleaned.strip()\n",
    "    \n",
    "    return text_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd7361",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31.3. Применяем функцию к нашему лемметизированному списку комментариев с заменой\n",
    "data['Предобработанный_текст'] = data['Предобработанный_текст'].apply(words_cleaner)\n",
    "data['Предобработанный_текст']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246b89a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_excel('Комментарии+Лемматизация+удаленённые_стоп-слова.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7425c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32. Добавляем столбец 'Предобработанный_текст' в таблицу с комментариями и ответами + транспонируем\n",
    "comments_replies = pandas.concat([comments_replies, data['Предобработанный_текст']], axis=1)\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f460af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Сохраняем\n",
    "comments_replies.to_excel('Все_комментарии(Абсолютно_все)+Комментарии+Лемматизация+удаленённые_стоп-слова.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefb074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 34. Импорт нужных библиотек для векторизации\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd6ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 35. Выгружаем только нужные столбцы\n",
    "comments_replies = pandas.read_excel('Все_комментарии(Абсолютно_все)+Комментарии+Лемматизация+удаленённые_стоп-слова.xlsx', usecols='G, S')\n",
    "comments_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 36. Создаём новый датафрейм и назначяем в нём нужный столбец из базы данных\n",
    "data = pandas.DataFrame()\n",
    "data.loc[:, 'Текст_на_векторизацию'] = comments_replies['Предобработанный_текст']\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c22ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 37. Выюираем пороговое значение для токенизации слов\n",
    "min_df = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e86127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38. Процесс создания матрицы векторизации\n",
    "cvect = CountVectorizer(min_df=min_df).fit(data['Текст_на_векторизацию'])\n",
    "# результат записываем в cmatrix\n",
    "cmatrix = cvect.transform(data['Текст_на_векторизацию'])\n",
    "# из cmatrix оформляем новый датафрейм\n",
    "cmatrix_df = pandas.DataFrame(cmatrix.toarray(), columns=cvect.get_feature_names_out(), index=data.index)\n",
    "cmatrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0575387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 39. Посмотрим на частотность токенов\n",
    "cmatrix_df[:].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 40. Добавляем матрицу к столбцам с исходным тектом\n",
    "cmatrix_df = pandas.concat([comments_replies, cmatrix_df], axis=1)\n",
    "cmatrix_df.to_csv('cmatrix_df.xlsx', index=False)\n",
    "cmatrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 41. Настройка класса TfidfVectorizer. Скрипт во многом схож с тем, что был в чанке № 16\n",
    "tfidfvect = TfidfVectorizer(min_df=min_df).fit(data['Текст_на_векторизацию'])\n",
    "# Результат записываем\n",
    "tfidfmatrix = tfidfvect.transform(data['Текст_на_векторизацию'])\n",
    "tfidfmatrix_df = pandas.DataFrame(tfidfmatrix.toarray(), columns=cvect.get_feature_names_out(), index=data.index)\n",
    "tfidfmatrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99c862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 42. Проверяем частотность\n",
    "tfidfmatrix_df[:].sum().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a739a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 43. Добавляем другую матрицу к столбцам с исходным тектом и сохраняем в формате csv\n",
    "tfidfmatrix_df = pandas.concat([comments_replies, tfidfmatrix_df], axis=1)\n",
    "tfidfmatrix_df.to_csv('tfidfmatrix_df.xlsx', index=False)\n",
    "tfidfmatrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f384c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 44. Импорт библиотек\n",
    "from randan.descriptive_statistics import ScaleStatistics\n",
    "from randan.dimension_reduction import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b66f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 45. Первичный анализ частотности токенов в наших базах данных\n",
    "data = cmatrix_df.drop(['snippet.textOriginal', 'Предобработанный_текст'], axis=1).sum()\n",
    "\n",
    "# Описательная статистика для интервальных значений\n",
    "data = pandas.DataFrame(data, columns=['Токенов_в_корпусе'])\n",
    "ss = ScaleStatistics(data[['Токенов_в_корпусе']])\n",
    "\n",
    "# Убираем столбцы, не являющиеся токенами\n",
    "data = cmatrix_df.drop(['snippet.textOriginal', 'Предобработанный_текст'], axis=1).T.sum()\n",
    "\n",
    "# Описательная статистика для интервальных значений\n",
    "data = pandas.DataFrame(data, columns=['Токенов_в_корпусе'])\n",
    "ss = ScaleStatistics(data[['Токенов_в_корпусе']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8a7aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 46. Убираем столбцы, не являющиеся токенами\n",
    "data = pandas.DataFrame()\n",
    "data = cmatrix_df.drop(['snippet.textOriginal', 'Предобработанный_текст'], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8075039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 47 Начинаем работать с PCA\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4433c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 48 Долгая операция -- подаём токены в PCA\n",
    "\n",
    "pca = pca.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793db2c",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "86% информации объясняется 2594 топиками (по критерию Кайзера)\n",
    "\n",
    "50% информации объясняется 275 топиками\n",
    "\n",
    "Медианное число токенов на документ, равно 6, объясняется 80 топиками = 25% информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 49 настройка модели\n",
    "pca = PCA(n_components=80, rotation='varimax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f982d979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50 Запуск обновлённой модели PCA\n",
    "pca = pca.fit(data, show_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7150a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51 Сохраняем матрицу с токенами в файл\n",
    "component_loadings_rotated = pca.component_loadings_rotated\n",
    "component_loadings_rotated.to_csv('component_loadings_rotated.xlsx')\n",
    "component_loadings_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37a93bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 Добавляем в матрицу документы топики документы в исходном и преобразованном состониях  для дальнейшей интерпретации\n",
    "scores = pca.transform(data)\n",
    "scores = pandas.concat([cmatrix_df[['snippet.textOriginal', 'Предобработанный_текст']], scores], axis=1)\n",
    "scores.to_csv('scores.xlsx')\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a13dd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 53. Импорт библиотек\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5869f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 54. Импорт баз данных\n",
    "component_loadings_rotated = pandas.read_csv('component_loadings_rotated.xlsx', index_col=0)\n",
    "scores = pandas.read_csv('scores.xlsx', index_col=0)\n",
    "display(component_loadings_rotated, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23465528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 55. Интепретируем результат в графиках\n",
    "loadings_threshold = 0.50\n",
    "n_tokens = 15\n",
    "n_docs = 15\n",
    "\n",
    "summary = pandas.DataFrame()\n",
    "errors = []\n",
    "for i in range(1, 81):\n",
    "    try:\n",
    "        print(f'Topic PC{i}_vrmx')\n",
    "        \n",
    "    # Документы-топики\n",
    "        data = scores[f'PC{i}_vrmx']\n",
    "        data.index = scores['snippet.textOriginal']\n",
    "        # Гистограмма изучаемой характеристики\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.hist(data.dropna(), color='grey')\n",
    "        plt.title(f\"Distribution of docs' scores across {data.name}\")\n",
    "        plt.xlabel(f'{data.name}')\n",
    "        plt.ylabel('Frequency');\n",
    "        # Боксплот изучаемой характеристики\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.boxplot(data.dropna()) \n",
    "        plt.title(f\"Distribution of docs' scores across {data.name}\")\n",
    "        plt.xticks([])\n",
    "        plt.ylabel(f'{data.name}');\n",
    "        plt.show()\n",
    "        # Полярные документы\n",
    "        topic_docs = pandas.concat([scores.sort_values([f'PC{i}_vrmx'], ascending=False)[['snippet.textOriginal', f'PC{i}_vrmx']].head(n_docs), scores.sort_values([f'PC{i}_vrmx'])[['snippet.textOriginal', f'PC{i}_vrmx']].head(n_docs)])\n",
    "        display(f'Документы на полюсах топика PC{i}_vrmx',\n",
    "                topic_docs)\n",
    "        \n",
    "    #Токены-топики\n",
    "        data = component_loadings_rotated[f'PC{i}_vrmx']\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.hist(data.dropna(), color='grey')\n",
    "        plt.title(f\"Distribution of tokens' scores across {data.name}\")\n",
    "        plt.xlabel(f'{data.name}')\n",
    "        plt.ylabel('Frequency');\n",
    "        # Боксплот изучаемой характеристики\n",
    "        plt.figure(figsize=(8,6))\n",
    "        plt.boxplot(data.dropna()) \n",
    "        plt.title(f\"Distribution of tokens' scores across {data.name}\")\n",
    "        plt.xticks([])\n",
    "        plt.ylabel(f'{data.name}');\n",
    "        plt.show()\n",
    "        # Полярные токены\n",
    "        topic_tokens = pandas.concat([data[data.abs() > loadings_threshold].sort_values(ascending=False).head(n_tokens),\n",
    "                                      data[data.abs() < -loadings_threshold].sort_values().head(n_tokens)])\n",
    "        display(f'Токены на полюсах топика PC{i}_vrmx',\n",
    "                topic_tokens)\n",
    "        \n",
    "    # Обработка полярных документов и токенов; запись в датафрейм\n",
    "        summary_additional = pandas.DataFrame()\n",
    "        \n",
    "        summary_plus = pandas.DataFrame()\n",
    "        if len(data[data > loadings_threshold].sort_values(ascending=False).head(n_tokens)) > 0:\n",
    "            summary_plus = topic_docs[topic_docs[f'PC{i}_vrmx'] > 0].round(3)\n",
    "            summary_plus.loc[:, 'Topic'] = i\n",
    "            summary_plus.loc[:, 'Токены'] = ', '.join(list(data[data > loadings_threshold].sort_values(ascending=False)\\\n",
    "                                            .head(n_tokens).index))\n",
    "            summary_plus.loc[:, 'Токены_Mean_Loading'] = data[data > loadings_threshold].sort_values(ascending=False)\\\n",
    "                                            .head(n_tokens).mean()\n",
    "            summary_plus.loc[:, 'Релевантность_теме_исследования'] = ''\n",
    "            summary_plus.loc[:, 'Название_топика'] = ''\n",
    "            \n",
    "        summary_minus = pandas.DataFrame()\n",
    "        if len(data[data < -loadings_threshold].sort_values(ascending=False).head(n_tokens)) > 0:\n",
    "            summary_minus = topic_docs[topic_docs[f'PC{i}_vrmx'] < 0].round(3)\n",
    "            summary_minus.loc[:, 'Topic'] = i\n",
    "            summary_minus.loc[:, 'Токены'] = ', '.join(list(data[data < -loadings_threshold].sort_values(ascending=False)\\\n",
    "                                            .head(n_tokens).index))\n",
    "            summary_minus.loc[:, 'Токены_Mean_Loading'] = data[data < -loadings_threshold].sort_values(ascending=False)\\\n",
    "                                            .head(n_tokens).mean()\n",
    "            summary_minus.loc[:, 'Релевантность_теме_исследования'] = ''\n",
    "            summary_minus.loc[:, 'Название_топика'] = ''\n",
    "            \n",
    "        summary_additional = pandas.concat([summary_plus, summary_minus])\n",
    "        summary_additional.columns = ['Текст_на_предобработку',\n",
    "                                      'scores',\n",
    "                                      'Topic',\n",
    "                                      'Токены',\n",
    "                                      'Токены_Mean_Loading',\n",
    "                                      'Релевантность_теме_исследования',\n",
    "                                      'Название_топика']\n",
    "        \n",
    "        summary = pandas.concat([summary, summary_additional])\n",
    "        \n",
    "    except:\n",
    "        print(f'Ошибка при обработке топика {i}')\n",
    "        errors.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0acb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 56 Смотрим summary\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2e4a45",
   "metadata": {},
   "source": [
    "Далее начался процесс ручной фильтрации топиков по релевантности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448e3792",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = pandas.read_excel('Топики.xlsx', index_col=0)\n",
    "topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de51563d",
   "metadata": {},
   "source": [
    "Все топики были проанализированы с точки зрения релевантности исследования"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a3133",
   "metadata": {},
   "source": [
    "# Выводы на основе части топиков:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828101ef",
   "metadata": {},
   "source": [
    "1. Было выявлено, что из-за того, что выдающиеся показатели количества комментариев было занято рекламой/подкастом/интервью основателя одного из финтех стартапов, отобранные 80 топиков представляют собой повторяющиеся аргументы\\доводы\\суждения и просто комментарии, разбитым на следующие темы: \n",
    "- Тайминг видео\n",
    "- Анализ услуг финтех стартапа\n",
    "- О личности основателя финтех стартапа\n",
    "- Отзыв об услагах\n",
    "- Комментарии об уровне владения английским ведущего\\спикера\\одного из участников дискуссии\n",
    "\n",
    "Итоги рефлексии по данному поводу: появилось желание сменить тему и поисковый запрос в процессе разметки релевантности топиков и их комментариев, однако оно было преодолено желанием довести проект с данным параметром поиска до логического завершения, не зависимо от того, каким по итогу окажется результат.\n",
    "\n",
    "2. Топики, по всей видимости, относящиеся к одному и тому же видео с выдающимися значениями по количеству комментариев, были определены при помощи предобработки текста как к наиболее подходящими по запросам исследования, что не является верным. Возникли сомнения в том, насколько релевантными целям исследования окажутся топики после разметки документов с учителем. \n",
    "\n",
    "3. При беглом осмотре, 5-ти топиков оказалось достаточно, чтобы выявить общу. картину из 80 топиков - комментарии повторяются и они косвенно относятся к теме финтеха, так как посвящены не самому являению\\процессу развития финтеха в России, а конкретному стартапу Револют, т.к. параметры видео, посвящённые данному видео, преобладают над количеством комментариев под другими видео, которые были отобраны при оценке релевантности базы данных с видео на тему финтеха. \n",
    "\n",
    "Вероятное решение проблемы, если будет желание в дальнейшем изучить эту тему: \n",
    "- Убрать из релевантных видео те, что относятся к обсуждению конкретного финтех стартапа\\продукта\n",
    "- Убрать из релевантных видео те, что относятся к обсуждению трейдинга, криптовалют и иных способов транзакции средств\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad0097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 57. Загружаем базу с cmatrix\n",
    "cmatrix_df = pandas.read_csv('cmatrix_df.xlsx')\n",
    "cmatrix_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce0a15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 58 Загружаем базу данных с разметкой релевантности топиков\n",
    "comments_replies_relevance = pandas.read_excel('Топики.xlsx', usecols='A, B, G', index_col=0)\n",
    "comments_replies_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a007b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 59. Выясним, есть ли в документе в столбце с релевантностью противоречия по показателю\n",
    "docS_index_unq = list(dict.fromkeys(comments_replies_relevance.index).keys())\n",
    "docS_index_unq.sort()\n",
    "\n",
    "mpping_cnflct = []\n",
    "for i in docS_index_unq:\n",
    "    print(f'Итерация {i}')\n",
    "    if (comments_replies_relevance.loc[i, 'Релевантность_теме_исследования'].mean() > 0)\\\n",
    "    & (comments_replies_relevance.loc[i, 'Релевантность_теме_исследования'].mean() < 1):\n",
    "        mpping_cnflct.append(i)\n",
    "print(f'Индексы документов, размеченных противоречиво: {mpping_cnflct}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 60. Удаление дубликатов\n",
    "comments_replies_relevance.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d479d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 61 Конкат двух датафреймов\n",
    "comments_replies_relevance = pandas.concat([comments_replies_relevance.drop_duplicates(), cmatrix_df], axis=1)\n",
    "comments_replies_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8269825f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 62 Проверяем совпадение столбцов двух датафреймов с комментариями\n",
    "comments_replies_relevance[['Текст_на_предобработку', 'snippet.textOriginal']].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcec854e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 63. Описательная статистика с исопльзованием ресурсов пандас\n",
    "comments_replies_relevance['Релевантность_теме_исследования'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ed45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 64 Убираем Текст_на_предобработку\n",
    "comments_replies_relevance = comments_replies_relevance.drop('Текст_на_предобработку', axis=1)\n",
    "comments_replies_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fad779f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 65 Сохраняем результат\n",
    "comments_replies_relevance.to_csv('Матрица_Релевантность.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b28b7fb",
   "metadata": {},
   "source": [
    "# Начало построение машинного обучение для текст-майнинга"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2bba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 66 Импорт пакета и базы данных и назначение зависимой переменной\n",
    "import randan\n",
    "data = pandas.read_csv('Матрица_Релевантность.csv', index_col=0)\n",
    "dpndnt = 'Релевантность_теме_исследования'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da85a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 67 Удаляем строки с пустыми (опустошёнными предобработкой) документами\n",
    "data = data[(data['Предобработанный_текст'].notna()) & (data['Предобработанный_текст'] != '')]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf85a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 68 Временно разделяем общий датафрейм на два, чтобы заменить все частоты, превышающие ноль на единицу\n",
    "data_docS = data[['Релевантность_теме_исследования', 'snippet.textOriginal', 'Предобработанный_текст']]\n",
    "data_tokenS = data.drop(['Релевантность_теме_исследования', 'snippet.textOriginal', 'Предобработанный_текст'], axis=1)\n",
    "tokenS = data_tokenS.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5a7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 69 Заменить все частоты, превышающие ноль на единицу\n",
    "data_tokenS[data_tokenS >= 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8fa9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70 Проверка\n",
    "print(f'{(data_tokenS == 0).sum().sum()} ячеек с нулевыми частотами')\n",
    "print(f'{(data_tokenS == 1).sum().sum()} ячеек с единичными частотами')\n",
    "print(f'{(data_tokenS > 1).sum().sum()} ячеек с частотами выше единичных')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c51aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 71 Снова объединяем части датафрейма\n",
    "data = pandas.concat([data_docS, data_tokenS], axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db278ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 72 Простейшая описатльная статистика из пандаса\n",
    "print(f\"{(data_docS[dpndnt].notna()).sum()} валидных значений зависимой переменной\")\n",
    "\n",
    "display('Распределение зависимой переменной на всех размеченных данных (в долях)',\n",
    "        data_docS[dpndnt].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a831f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 73 Делим датафрейм на размеченный и неразмеченный\n",
    "data_tagged = data[data[dpndnt].notna()]\n",
    "data_untagged = data[data[dpndnt].isna()]\n",
    "display(data_tagged, data_untagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387dfcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 74 Делим учительский датафрейм на обучающий (75%) и тестовый (25%) выборки\n",
    "data_tagged_training = data_tagged.sample(frac=0.75, random_state=1)\n",
    "data_tagged_test = data_tagged.drop(data_tagged_training.index)\n",
    "display('Обучающая выборка:', data_tagged_training, 'Тестовая выборка:', data_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd40cd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75 Снова простейшая описатльная статистика из пандаса\n",
    "display('Распределение зависимой переменной в обучающей выборке (в долях)',\n",
    "        data_tagged_training[dpndnt].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccd0988",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tagged_training[tokenS].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tagged_training[tokenS].var().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8d9c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tagged_training[tokenS].var().sort_values() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b47961",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenS_out = list(data_tagged_training[tokenS].var()[data_tagged_training[tokenS].var() == 0].index)\n",
    "print(f'Токены без дисперсии -- исключение: {tokenS_out}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab0472a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Проверка: дисперсия токены \"{tokenS_out[0]}\" равна', data_tagged_training[tokenS].var()[tokenS_out[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6c548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 76 Список предикторов (токены с дисперсией в обучающей выборке)\n",
    "tokenS_in = list(data_tagged_training[tokenS].var()[data_tagged_training[tokenS].var() != 0].index)\n",
    "print(f'Остаются {len(tokenS_in)} токенов: {tokenS_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f8e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 77. Начинаем с малого - начинаем разметку текстов\n",
    "import os\n",
    "os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin/'\n",
    "chaid = randan.tree.CHAIDClassifier(max_depth=None,\n",
    "                                    min_child_node=10,\n",
    "                                    min_parent_node=0).fit(data_tagged_training,\n",
    "                                                           dpndnt,\n",
    "                                                           tokenS_in,\n",
    "                                                           test_data=data_tagged_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff7f35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 78. Обученным деревом размечаем неразмеченные документы\n",
    "predictionS = chaid.predict(data_untagged)\n",
    "print(f'{predictionS.sum()} релевантных документов среди размеченных деревом')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a2d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80. Конкат\n",
    "data_untagged = pandas.concat([predictionS, data_untagged], axis=1)\n",
    "data_untagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc8b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 81. Продолжаем конкатинацию\n",
    "data = pandas.concat([data_untagged, data_tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301bd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 82. Выделяем только релевантные документы\n",
    "data_relevant = data[(data[dpndnt] == 1) | (data[predictionS.columns[0]] == 1)]\n",
    "data_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8948adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 83. Сохраняем обновлённый датафрейм с результатами применения машинного обучения с учителем\n",
    "data.to_csv(f'Матрица_Релевантность_SL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412c2de8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
